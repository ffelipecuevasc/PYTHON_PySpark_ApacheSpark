# PYTHON_PySpark_ApacheSpark

Este repositorio contiene proyectos desarrollados en Python que permiten practicar la gestiÃ³n y anÃ¡lisis de datos con Apache Spark, utilizando Jupyter Notebooks en Google Colab como entorno interactivo.

# ğŸ“˜ Contenido del Repositorio

El repositorio incluye dos notebooks diseÃ±ados de forma pedagÃ³gica para introducir progresivamente conceptos clave del ecosistema Apache Spark:

## 1ï¸âƒ£ Apache Spark RDD.ipynb

ConexiÃ³n a Apache Spark y trabajo con RDDs

Este notebook explica:

- CÃ³mo inicializar una sesiÃ³n de Spark (SparkSession) en Google Colab.
- CÃ³mo cargar datos desde un archivo CSV.
- CÃ³mo asignar los datos a un RDD (Resilient Distributed Dataset).
- Casos de uso simples para entender el procesamiento distribuido basado en RDDs.

ğŸ¯ Ideal para quienes estÃ¡n comenzando con PySpark y desean comprender el enfoque mÃ¡s bÃ¡sico de procesamiento de datos.

## 2ï¸âƒ£ Apache Spark DF SQL.ipynb

Spark SQL, DataFrames y Motores de OptimizaciÃ³n

Este notebook avanza hacia un enfoque mÃ¡s estructurado con:

- ConfiguraciÃ³n de SparkSession.
- Carga de datos desde CSV directamente en un DataFrame.
- Uso de consultas SQL sobre DataFrames con spark.sql().
- CreaciÃ³n de funciones en Python registradas como UDFs (User Defined Functions) en el contexto SQL.
- ExplicaciÃ³n de los motores de optimizaciÃ³n internos de Apache Spark:
  - ğŸ” **Catalyst:** optimizaciÃ³n lÃ³gica y reescritura de planes de ejecuciÃ³n SQL.
  - âš™ï¸ **Tungsten:** mejoras fÃ­sicas a nivel de memoria y ejecuciÃ³n.

ğŸ¯ Este segundo notebook es ideal para quienes desean avanzar en el uso eficiente de Spark SQL y comprender cÃ³mo Spark optimiza sus operaciones internas.

## 3ï¸âƒ£ Apache Spark Streaming.ipynb

SimulaciÃ³n de procesamiento en tiempo real con Spark Structured Streaming. Este notebook introduce el procesamiento de flujos de datos simulados utilizando Apache Spark Streaming en Google Colab, ideal para comprender cÃ³mo funciona el modelo de micro-batches en contextos reales.

En este proyecto aprenderÃ¡s a:

- âš™ï¸ Configurar un entorno Spark Streaming desde cero en Google Colab.
- ğŸ“ Simular la llegada de datos en tiempo real usando archivos CSV copiados como micro-lotes.
- ğŸ§± Definir esquemas estructurados (`StructType`) para la lectura de datos en modo `readStream`.
- ğŸ“Š Aplicar transformaciones en tiempo real como `groupBy` y `count` sobre flujos continuos.
- ğŸ–¨ï¸ Utilizar `foreachBatch` para imprimir resultados procesados por cada micro-batch.
- ğŸ§  Comprender conceptos clave como:
  - ğŸ“¦ **Micro-batch:** pequeÃ±as porciones de datos procesadas a intervalos definidos.
  - ğŸ“ **Checkpoint:** carpeta donde Spark guarda el estado de ejecuciÃ³n para tolerancia a fallos.
  - ğŸ”„ **Diferencias entre batch y streaming:** ejecuciÃ³n tradicional vs. flujo continuo.

ğŸ¯ Este tercer notebook es especialmente Ãºtil para quienes desean iniciarse en el **procesamiento de datos en tiempo real** de manera progresiva, sin depender de herramientas externas como Kafka o Flink.
