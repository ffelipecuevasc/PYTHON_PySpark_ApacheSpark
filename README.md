# PYTHON_PySpark_ApacheSpark

Este repositorio contiene proyectos desarrollados en Python que permiten practicar la gestiÃ³n y anÃ¡lisis de datos con Apache Spark, utilizando Jupyter Notebooks en Google Colab como entorno interactivo.

# ğŸ“˜ Contenido del Repositorio

El repositorio incluye **cuatro** notebooks diseÃ±ados de forma pedagÃ³gica para introducir progresivamente conceptos clave del ecosistema Apache Spark:

## 1ï¸âƒ£ [Apache Spark RDD.ipynb]

**ConexiÃ³n a Apache Spark y trabajo con RDDs**

Este notebook explica:

- CÃ³mo inicializar una sesiÃ³n de Spark (`SparkSession`) en Google Colab.
- CÃ³mo cargar datos desde un archivo CSV.
- CÃ³mo asignar los datos a un RDD (*Resilient Distributed Dataset*).
- Casos de uso simples para entender el procesamiento distribuido basado en RDDs.

ğŸ¯ Ideal para quienes estÃ¡n comenzando con PySpark y desean comprender el enfoque mÃ¡s bÃ¡sico de procesamiento de datos.

---

## 2ï¸âƒ£ [Apache Spark DF SQL.ipynb]

**Spark SQL, DataFrames y Motores de OptimizaciÃ³n**

Este notebook avanza hacia un enfoque mÃ¡s estructurado con:

- ConfiguraciÃ³n de `SparkSession`.
- Carga de datos desde CSV directamente en un `DataFrame`.
- Uso de consultas SQL sobre DataFrames con `spark.sql()`.
- CreaciÃ³n de funciones en Python registradas como UDFs (*User Defined Functions*) en el contexto SQL.
- ExplicaciÃ³n de los motores de optimizaciÃ³n internos de Apache Spark:
  - ğŸ” **Catalyst:** optimizaciÃ³n lÃ³gica y reescritura de planes de ejecuciÃ³n SQL.
  - âš™ï¸ **Tungsten:** mejoras fÃ­sicas a nivel de memoria y ejecuciÃ³n.

ğŸ¯ Este segundo notebook es ideal para quienes desean avanzar en el uso eficiente de Spark SQL y comprender cÃ³mo Spark optimiza sus operaciones internas.

---

## 3ï¸âƒ£ [Apache Spark Streaming.ipynb]

**SimulaciÃ³n de procesamiento en tiempo real con Spark Structured Streaming**

Este notebook introduce el procesamiento de flujos de datos simulados utilizando Apache Spark Streaming en Google Colab, ideal para comprender cÃ³mo funciona el modelo de *micro-batches* en contextos reales.

En este proyecto aprenderÃ¡s a:

- âš™ï¸ Configurar un entorno Spark Streaming desde cero en Google Colab.
- ğŸ“ Simular la llegada de datos en tiempo real usando archivos CSV copiados como micro-lotes.
- ğŸ§± Definir esquemas estructurados (`StructType`) para la lectura de datos en modo `readStream`.
- ğŸ“Š Aplicar transformaciones en tiempo real como `groupBy` y `count` sobre flujos continuos.
- ğŸ–¨ï¸ Utilizar `foreachBatch` para imprimir resultados procesados por cada micro-batch.
- ğŸ§  Comprender conceptos clave como:
  - ğŸ“¦ **Micro-batch:** pequeÃ±as porciones de datos procesadas a intervalos definidos.
  - ğŸ“ **Checkpoint:** carpeta donde Spark guarda el estado de ejecuciÃ³n para tolerancia a fallos.
  - ğŸ”„ **Diferencias entre batch y streaming:** ejecuciÃ³n tradicional vs. flujo continuo.

ğŸ¯ Este tercer notebook es especialmente Ãºtil para quienes desean iniciarse en el **procesamiento de datos en tiempo real** de manera progresiva, sin depender de herramientas externas como Kafka o Flink.

---

## 4ï¸âƒ£ [4 - Apache Spark MLlib.ipynb]

**Aprendizaje automÃ¡tico con Spark MLlib (API DataFrame)**

IntroducciÃ³n prÃ¡ctica al aprendizaje automÃ¡tico con un flujo completo desde el preprocesamiento hasta la evaluaciÃ³n y persistencia del modelo.

En este notebook trabajarÃ¡s con:

- ğŸ§¹ **PreparaciÃ³n de datos para ML**
  - Limpieza bÃ¡sica y tratamiento de valores nulos.
  - Manejo de variables categÃ³ricas con `StringIndexer` y `OneHotEncoder`.
  - Ensamble de caracterÃ­sticas con `VectorAssembler`.
  - Escalamiento/normalizaciÃ³n con `StandardScaler`.

- ğŸ§± **Pipelines de ML reproducibles**
  - CreaciÃ³n de un `Pipeline` con *stages* encadenados (indexado â†’ codificaciÃ³n â†’ ensamblado â†’ escalado â†’ estimador).
  - Uso de `PipelineModel` para transformar datos de entrenamiento y prueba de forma consistente.

- ğŸ¤– **Modelos incluidos**
  - **RegresiÃ³n:** `LinearRegression`, `RandomForestRegressor` (visiÃ³n general).
  - **ClasificaciÃ³n:** `LogisticRegression`, `RandomForestClassifier`.
  - InterpretaciÃ³n bÃ¡sica: coeficientes y *feature importances*.

- ğŸ§ª **Entrenamiento, validaciÃ³n y evaluaciÃ³n**
  - DivisiÃ³n `train/test` con semilla fija para reproducibilidad.
  - MÃ©tricas con `RegressionEvaluator` (RMSE, RÂ²) y `MulticlassClassificationEvaluator` (accuracy, F1).
  - VisualizaciÃ³n de resultados y predicciones con `transform()` y `show(truncate=False)`.

ğŸ¯ Este cuarto notebook es ideal para quienes desean **dar el salto del procesamiento de datos al modelado de ML** en Spark, utilizando flujos reproducibles y escalables que siguen las mejores prÃ¡cticas de la API de MLlib.
